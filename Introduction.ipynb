{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduces some basic concepts in tensorflow (TF) and presents the way that I think about my TF models.\n",
    "\n",
    "Generally, my TF scripts take the following form:\n",
    "\n",
    "    - Boilerplate (import and preprocess data, etc.)\n",
    "    - Tensorflow Model (graph)\n",
    "    - Developing loss functions/training algorithm\n",
    "    - Training loop\n",
    "    \n",
    "The second and third item are where the most TF magic usually happens for me. Lets focus in on the second item the Tensorflow Model (graph).\n",
    "\n",
    "A model (i.e. ‘computational graph’) is composed on three main parts:\n",
    "\n",
    "    - Inputs (usually in the form of `placeholder` variables to be filled in later, we'll introduce this in a moment)\n",
    "    - Tensorflow operations, which are composable functions that take `Tensor` objects as inputs and return `Tensor` objects as outputs.\n",
    "    - Outputs of the model (i.e. network response, etc.). These are actually just more Tensor variables, outputs of operations, but there are usually a few distinguished ones you care about.\n",
    "\n",
    "Before we get to coding, I just want to make one more point, which is to distinguish `Tensor` objects from `Variables`. When I am working with TF I generally think about two types of tensor object:\n",
    "\n",
    "    - `Tensors`, which is a general term for many tensorflow objects, but I usualy abuse language by using this to refer specifically to symbolic expressions: TF expressions whose value depends on a yet-to-be-specified input (for example, the output of a network depends on its inputs, which usually are not specified till run-time).\n",
    "    - `Variables`, which specifically refers to tensor objects with a specific value. Generally these values are stored on the GPU, which is important because it is expensive to load data onto/off of the GPU. A good example of `Variable` objects are network weights. Their values are persistent, and can be used and updated entirely on the GPU, no need to offload them to do gradient updates, etc.\n",
    "    \n",
    "Ok. To try and make this distinction clear, lets look at an example. The following code defines an affine transformation (operation with a matrix, plus a vector offset) on a batch of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "vector_size = 100\n",
    "\n",
    "input_vectors = tf.placeholder(tf.float32,shape = (batch_size,vector_size))\n",
    "\n",
    "W = tf.Variable(np.float32(np.random.randn(vector_size,vector_size)))\n",
    "t = tf.Variable(np.float32(np.random.randn(vector_size)))\n",
    "\n",
    "output = tf.tensordot(input_vectors,W,axes = [[1],[1]]) + tf.expand_dims(t,0)\n",
    "\n",
    "mean_out = tf.reduce_mean(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `input_vectors` is a `placeholder`, this means that we can specify its value later on. On the other hand, W and t are `Variable` objects, and I have given the `Variable` function an explicit value (the output of a numpy RNG). This variable object will be stored on the GPU (or onboard memory) and has a determined value. \n",
    "\n",
    "But how do we actually compute the value of `mean_out`? If we just try to print it we get garbage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just telling us that `output` is a `Tensor` object. Of course, `mean_out` __can't__ have a value yet, because it depends on `input_vectors`, which doesn't yet have a value!\n",
    "\n",
    "In order to compute it we need to start a tensorflow `Session`, and tell tensorflow what `input_vectors` is. That looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "data = np.random.randn(batch_size,vector_size)\n",
    "\n",
    "out = sess.run(mean_out,{input_vectors:data})\n",
    "print(\"The average output is: {}\".format(out))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first three lines start a tensorflow `Session`. This is basically spinning up the GPU and getting tensorflow ready to compute stuff. We also create a `global_varialbes_initializer`. The reason for this is that the variables `W` and `t`, while they do have an explicit value, the actual numbers have not yet been plugged into the GPU memory (or something like that). the `global_variables_initializer`, prepares the variables for use by a tensorflow session. Try commenting out the third line from this cell so the `init` operation isn't performed and see what happens!\n",
    "\n",
    "The next line `data = ...` is just me generating an explicit value that we will feed for `input_vectors`.\n",
    "\n",
    "The line `out = ...` is the powerhouse here. `sess.run` is the function that performs all the tensorflow computations (for v 1.x). The first argument is what we want to compute (here `mean_out`) and the second argument is a `dict` that we can use to feed in values for undetermined tensors. The keys of the `dict` are the tensor objects that need to be specified, and the entries are the value to assign to them. (in the example above, we assign the placeholder `input_vectors` with the value `data`).\n",
    "\n",
    "Lastly we close the session (we could leave it open and reuse it below, but I wanted to make each cell as independent as possible).\n",
    "\n",
    "One more example shows that we can specify any tensor in the graph (not just the `input_vectors`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "data = np.random.randn(batch_size,vector_size)\n",
    "\n",
    "out = sess.run(mean_out,{output:data})\n",
    "print(\"The average output is: {}\".format(out))\n",
    "print(\"The average data is: {}\".format(np.mean(data)))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell is identical to the last, but we specify the `output` tensor instead of `input_vectors`. Sicne we specify the `output` we no longer need `input_vectors` to compute `mean_out` because once `output` is specified, we can unambiguously determine `mean_out`. \n",
    "\n",
    "As one more example, we can specify `W` explicitly if we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "data = np.random.randn(batch_size,vector_size)\n",
    "Wtemp1 = np.random.randn(vector_size,vector_size)\n",
    "Wtemp2 = np.random.randn(vector_size,vector_size)\n",
    "\n",
    "out = sess.run(mean_out,{input_vectors:data,W:Wtemp1})\n",
    "print(\"The average output is: {}\".format(out))\n",
    "\n",
    "out = sess.run(mean_out,{input_vectors:data,W:Wtemp2})\n",
    "print(\"The average output is: {}\".format(out))\n",
    "\n",
    "out = sess.run(mean_out,{input_vectors:data})\n",
    "print(\"The average output is: {}\".format(out))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in this cell, we specify W to be two different values, and then in the third `sess.run` we leave it unspecified, and it defaults back to the value we initialized it to above. Cool!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what you've seen here is a simple example of an input which is acted upon by some variables stored on the GPU, and produces an output. There were four examples of operations given here: the `tensordot` that applies `W` to the input, the `tf.expand_dims` which reshaped `t`, the addition of the two terms to produce `output`, and the `tf.reduce_mean` operation which took the average output. These operations produce tensors as ouput, and computing them with `sess.run` performs that operation. In all the examples above, the operations take symbolic expressions and produce more symbolic expressions, but operations can also do other things, such as change the value of `Variable` objects. \n",
    "\n",
    "As an example of this, lets make an operation which shifts `W` by a specified ammount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_W = tf.assign(W,W+.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the expression `change_W` represents `W` *after* it has been shifted. to actually apply the shift we have to `sess.run` the tensor `change_W`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "w = sess.run(W)\n",
    "print(\"shift 0: {}\".format(w.mean()))\n",
    "\n",
    "for k in range(5):\n",
    "    _ = sess.run(change_W)\n",
    "    w = sess.run(W)\n",
    "    print(\"shift {}: {}\".format(k+1,w.mean()))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now we've changed the value of `W`. Cool! The thing that confuses me about this sometimes is that the tensor `change_W` contains the instructions to applly the shift. For a long time I thought of the tensor objects as merely representing not-yet-determined values of mathematical expressions, but they are more than that. They contain a set of instructions which can include manipulating variables, combining tensors, and more. \n",
    "\n",
    "As a small application, lets use the `tf.assign` function, and gradients, to solve a linear set of equations using gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = .001\n",
    "\n",
    "target_vectors = tf.placeholder(tf.float32,(batch_size,vector_size))\n",
    "error = tf.reduce_mean((target_vectors - output)**2)\n",
    "grad = tf.gradients(error,W)\n",
    "grad_step = tf.assign(W, W - lr * grad[0])\n",
    "\n",
    "data = np.random.randn(batch_size,vector_size)\n",
    "target = np.random.randn(batch_size,vector_size)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "err = sess.run(error,{input_vectors:data,target_vectors:target})\n",
    "\n",
    "print(err)\n",
    "\n",
    "for k in range(5000):\n",
    "    w = sess.run(grad_step,{input_vectors:data,target_vectors:target})\n",
    "    if k%500 == 0:\n",
    "        err = sess.run(error,{input_vectors:data,target_vectors:target})\n",
    "        print(\"Step {} error: {}\".format(k,err))\n",
    "        \n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works! Or at least the error goes down, great. But so slowly!?\n",
    "\n",
    "What you saw here was defining an error (RMS error between the `output` and a new placeholder`target_vectors`), then computation of the gradient of the error w.r.t. `W`, and then the creation of an operator which takes a single gradient descent step. Then there is the boilerplate of starting a session, and then a \"training loop\" that perform gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, the last thing I want to show you is tensorflows built in tools for optimization, which will work much better than this lame gradient descent. In the previous cell I defined by own gradient descent operation using the `gradients` function. Tensorflow, however, has many algorithms pre-made in the `train` module. We can perform the same optimization using the `Adam` algorithm in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = .001\n",
    "\n",
    "target_vectors = tf.placeholder(tf.float32,(batch_size,vector_size))\n",
    "error = tf.reduce_mean((target_vectors - output)**2)\n",
    "\n",
    "adam = tf.train.AdamOptimizer(learning_rate = lr)\n",
    "grad_step = adam.minimize(error)\n",
    "\n",
    "data = np.random.randn(batch_size,vector_size)\n",
    "target = np.random.randn(batch_size,vector_size)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "err = sess.run(error,{input_vectors:data,target_vectors:target})\n",
    "\n",
    "print(err)\n",
    "\n",
    "for k in range(5000):\n",
    "    w = sess.run(grad_step,{input_vectors:data,target_vectors:target})\n",
    "    if k%500 == 0:\n",
    "        err = sess.run(error,{input_vectors:data,target_vectors:target})\n",
    "        print(\"Step {} error: {}\".format(k,err))\n",
    "        \n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, adam converged much faster. Although I have trouble taking this advice sometimes: it is usually better to find someone else's implementation than to create your own :P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! I hope you have some ideas now of how to do stuff in TF. Check out the other notebooks for full-fledged examples of networks that learn to recognize MNIST digits. \n",
    "\n",
    "Some stuff for you to try here is:\n",
    "\n",
    "    - Change the error function and see how it effects learning (for example, what about using `tf.reduce_max` instead of `tf.reduce_mean`?)\n",
    "    - Modify the code so that it perform linear regression on the batch, instaed of just solving a linear regression problem, by specifying the \"true\" transofrmation and noise, generating batches of data, and fitting `W` and `t`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The infamous XOR problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are going to create a simple network to solve the xor problem and train it, and we will play along the way with pitfalls we might fall into.\n",
    "\n",
    "First we prepare the data, we are just going to give the four noiseless data points to make life easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = np.float32([[a,b] for a in [0,1] for b in [0,1]])\n",
    "raw_output = np.float32([[bool(a)^bool(b)] for a,b in raw_data])\n",
    "\n",
    "\n",
    "ind = np.random.choice(4,100,replace = True)\n",
    "noisy_data = np.float32([raw_data[k] + np.random.randn(2)/15 for k in ind])\n",
    "noisy_output = np.float32([raw_output[k] for k in ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter([y[0] for x,y in enumerate(noisy_data) if noisy_output[x] == 1],[y[1] for x,y in enumerate(noisy_data) if noisy_output[x] == 1],label = \"1\")\n",
    "plt.scatter([y[0] for x,y in enumerate(noisy_data) if noisy_output[x] == 0],[y[1] for x,y in enumerate(noisy_data) if noisy_output[x] == 0],label = \"0\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data all set up, lets build the network. The next cell allows you to set the number of layers, number of hidden nodes in each layer, the learning rate and the activation of the hidden layers.\n",
    "\n",
    "We'll go through it as a group, but try running it with the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlayers = 2\n",
    "nhidden = 2\n",
    "learning_rate = .1\n",
    "\n",
    "#activation = tf.nn.sigmoid\n",
    "activation = tf.nn.tanh\n",
    "\n",
    "inp = tf.placeholder(tf.float32,shape = [None,2])\n",
    "out = tf.placeholder(tf.float32,shape = [None,1])\n",
    "\n",
    "net = inp\n",
    "\n",
    "for k in range(nlayers-1):\n",
    "        net = tf.layers.dense(net,nhidden,activation = activation)\n",
    "        \n",
    "net = tf.layers.dense(net,1,activation = tf.nn.sigmoid)\n",
    "\n",
    "loss = tf.reduce_mean((net - out)**2)\n",
    "\n",
    "OPT = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
    "#OPT = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "\n",
    "train = OPT.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "sess = tf.Session(config = config)\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop is in the next cell. You can specify how many steps to train for (`nepc`) and now often to report. It defaults to report ten times throughout training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nepc = 4000\n",
    "report = nepc // 10\n",
    "batch_size = 2\n",
    "\n",
    "loss_list = []\n",
    "noiseless_list = []\n",
    "for k in range(nepc):\n",
    "    tempind = np.random.choice(len(noisy_data),batch_size,replace = False)\n",
    "    \n",
    "    tempdat = [noisy_data[k] for k in tempind]\n",
    "    tempout = [noisy_output[k] for k in tempind]\n",
    "    \n",
    "    _,current_loss = sess.run([train,loss],{inp:tempdat,out:tempout})\n",
    "    loss_list.append(current_loss)\n",
    "    \n",
    "    test_loss = sess.run(loss,{inp:raw_data,out:raw_output})\n",
    "    noiseless_list.append(test_loss)\n",
    "\n",
    "    if k%report == 0:\n",
    "        print(\"{}\\tloss:{}\".format(k,test_loss))\n",
    "        \n",
    "x = np.linspace(0,1,20)\n",
    "feature_map = sess.run(net,{inp:[[a,b] for a in x for b in x]})\n",
    "feature_map = np.squeeze(np.reshape(feature_map,[len(x),len(x),1]))\n",
    "\n",
    "print(\"Done! Final loss: {}\".format(current_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we visualize the loss throughout training and the reponse of the network to different inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_list,label = \"train loss\")\n",
    "plt.plot(noiseless_list,label = \"test loss\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"RMS loss\")\n",
    "plt.legend\n",
    "plt.ylim([0,np.max(loss_list)])\n",
    "plt.show()\n",
    "\n",
    "plt.pcolor(x,x,feature_map)\n",
    "plt.xlabel(\"bit 1\")\n",
    "plt.ylabel(\"bit 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! You should try running this a few times with different paramters.\n",
    "\n",
    "For the remainder of the class, work with people around you to do the following:\n",
    "  * try messing with the batch size and learning rate, what improves/impairs learning?\n",
    "  * Try changing the nubmer of layers and hidden neurons, how does this change the speed at which the network converges?\n",
    "  * Change the inputs to include multiple samples from each group with some noise.\n",
    "  * Change the inputs to fill the entire region in the corresponding corner of the [0,1,0,1] square. i.e. instead provide data so the network learns to compute xor(sign(a - .5),sign(b - .5)). How does this change the ability of the netowrk to converge? Why?\n",
    "  * For the remainder of the time, just play around: can you generate a new set of tensorflow code to compute something for you? Come up with a function/series of computations you want to perform and generate code to do it for you in tensorflow. Remember that the online TF documentation will get you a long way if you don't know how to do something!\n",
    "  \n",
    "And please if there is anything you are curious about, ask me!\n",
    "  \n",
    "If you really have this all figured out, a fun project might be to code up a tenforflow script that implements learning and recall in the hopfield network ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
