{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import tensorflow, numpy and matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell defines a function to load in the MNIST csv files I downloaded and zipped (you gotta unzip them yourself), and then loads the data. This takes a minute because the method I am using to load data is slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(f):\n",
    "    F = open(f,\"r\")\n",
    "    out = []\n",
    "    label = []\n",
    "    for l in F:\n",
    "        temp = [int(t) for t in l.split(\",\")]\n",
    "        out.append(temp[1:])\n",
    "        label.append(temp[0])\n",
    "        \n",
    "    return np.array(out),np.array(label)\n",
    "\n",
    "train_data,train_label = load(\"./mnist_train.csv\")\n",
    "test_data,test_label = load(\"./mnist_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize a digit, becasue it's always good to look at your data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(np.reshape(test_data[0],[28,28]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "data_size = len(test_data[0])\n",
    "learning_rate = .001\n",
    "n_epochs = 10000\n",
    "n_labels = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the data. Note that we retain `m` and `s` so that we can normalize the test data in exactly the same way!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.mean(train_data)\n",
    "s = np.std(train_data)\n",
    "\n",
    "normed_data = (train_data - m)/s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These placeholders represent the inputs and labels for the data, which we will fill in later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "input_data = tf.placeholder(tf.float32,shape = (None,data_size))\n",
    "labels = tf.placeholder(tf.float32,shape = (None,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This defines the neural network layer by layer. Note that the last layer uses a linear activation function to generate *unnormalized log probabilities*. This is beause the loss function I used assumes the inputs are as such, and applies a softmax internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = tf.layers.dense(input_data,256,activation = tf.nn.relu,name = \"L1\")\n",
    "layer2 = tf.layers.dense(layer1,256,activation = tf.nn.relu,name = \"L2\")\n",
    "layer3 = tf.layers.dense(layer2,256,activation = tf.nn.relu,name = \"L3\")\n",
    "\n",
    "logits = tf.layers.dense(layer3,n_labels,activation = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss: average cross entropy loss between the NN output and the 1-hot encoded labels. Also create an ADAM optimizer and generate the training operation `train`.\n",
    "\n",
    "The variable `scope` controls the parameters that are trained. If `scope = None` then all parameters are trained. by making the `scope` equal to a lyaer name (see above) you restrict training to only those variables. Play with this! In particular, the network learns much better if only `L1` weights are trained than if only `L3` weights are trained! Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels = labels, logits = logits))\n",
    "\n",
    "scope = \"\"\n",
    "\n",
    "regularize = .0001*tf.reduce_sum([tf.reduce_sum(t**2) for t in tf.trainable_variables(scope = scope)])\n",
    "\n",
    "adam = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "\n",
    "train = adam.minimize(loss + regularize,var_list = tf.trainable_variables(scope = scope))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These helper functions take in a set of labels and generates a 1-hot encoding of them (`make_1hot`) and calculate the error based on the MAP estimate of the label (`MNIST_err`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_1hot(l):\n",
    "    out = np.zeros(shape = (len(l),n_labels))\n",
    "    for k in range(len(l)):\n",
    "        out[k][l[k]] = 1\n",
    "        \n",
    "    return out\n",
    "\n",
    "def MNIST_err(l,p):\n",
    "    out = []\n",
    "    for k in range(len(l)):\n",
    "        if l[k][np.argmax(p[k])] == 1:\n",
    "            out.append(1)\n",
    "        else:\n",
    "            out.append(0)\n",
    "            \n",
    "    return np.mean(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a session (on CPU) and initialize all the TF variables. That is, the weights and biases of the network, and the adam momentum variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training loop! \n",
    "\n",
    "At each step, pick a random batch of training data, run the training operation substituting in the batch of data for the placeholders above. \n",
    "\n",
    "Also periodically run the test data and record the train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "results = []\n",
    "print_freq = 500\n",
    "test_freq = 10\n",
    "normed_test = (test_data - m)/s\n",
    "test_label_1h = make_1hot(test_label)\n",
    "t1 = time.time()\n",
    "print(\"epoch\\ttrain loss\\ttest loss\\ttime taken\")\n",
    "for k in range(n_epochs):\n",
    "    b_index = np.random.choice(range(len(test_data)),batch_size,replace = False)\n",
    "    batch_data = normed_data[b_index]\n",
    "    batch_labels = make_1hot(train_label[b_index])\n",
    "    \n",
    "    _,train_loss,train_logits = sess.run([train,loss,logits],{input_data:batch_data,labels:batch_labels})\n",
    "    \n",
    "    if k%test_freq == 0:\n",
    "        test_loss,test_logits = sess.run([loss,logits],{input_data:normed_test,labels:test_label_1h})\n",
    "        results.append([k,MNIST_err(test_label_1h,test_logits),MNIST_err(batch_labels,train_logits)])\n",
    "    if k%print_freq == 0:        \n",
    "        print(\"{}\\t{}\\t{}\\t{}\".format(k,train_loss,test_loss,time.time() - t1))\n",
    "        t1 = time.time()\n",
    "\n",
    "results = np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, visualize the results of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(1+results[:,0],1-results[:,2],label = \"Train\")\n",
    "plt.plot(1+results[:,0],1-results[:,1],label = \"Test\")\n",
    "plt.plot(1+results[:,0],np.ones_like(results[:,1])*.9,\"k--\",label = \"Chance\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Gradient Step\")\n",
    "plt.ylabel(\"Error (%)\")\n",
    "plt.xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things for YOU to play with:\n",
    "\n",
    "    - Adjust learning rate, number/size of hidden layers, loss function (what about rms error?).\n",
    "    - Add drop out between dense layers.\n",
    "    - Augment the dataset: include images flipped horizontally/vertically. Add data modified by affine transformations. Add Noise.\n",
    "    - Try other activation functions.\n",
    "    - Try making the network convolutional.\n",
    "    - Restrict the parameters that are trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
